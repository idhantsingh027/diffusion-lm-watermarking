## What is watermarking?

Watermarking embeds a hidden, statistically detectable signal into model outputs.
The goal is to verify if text came from an AI system while keeping it fluent.
This protects models from IP violations (copyright, unauthorized use, modification).
Model owners should be aware of these risks.

## What are diffusion models (DMs)?

Diffusion models generate images or text through a sequential denoising process.
Unlike autoregressive models (e.g., GPT) that generate one token at a time, diffusion models update all tokens jointly over multiple steps.
They are not secure by design and can be misused or stolen, leading to IP loss or abuse (e.g., fake images/videos).

## Why watermark diffusion models?

As AI text becomes widespread, it is harder to distinguish human from AI content.
Watermarking embeds a signature with minimal quality loss and stronger robustness against removal.

## Goal

Diffusion LMs generate via iterative denoising (not left-to-right), so classical watermarking does not directly apply.
We design watermarking schemes that respect the trade-offs between detectability, robustness, and text quality.

## New insights (implementation-focused)

### Timestep conditioning

- Desired objective: $p_\theta(x_0 \mid x_t, t)$ (or $p_\theta(x_{t-1} \mid x_t, t)$ in stricter forms).
- If the model never sees $t$, all noise levels collapse into one task (weaker baseline).
- Conditioning on $t$ lets the denoiser adapt to corruption level.

### D3PM-style reverse process

- Forward: mask tokens with cumulative rate $p_t$.
- Reverse: **monotonic unmasking** from $t \to t-1$ so mask rate drops from $p_t$ to $p_{t-1}$.
- This differs from Mask-Predict, which re-masks already generated tokens.

### Stochastic final fill (important for watermarking)

- Final tokens should be sampled, not argmax.
- Sampling preserves diversity and keeps the watermark signal.

## D3PM-style Discrete Diffusion with Token Masking

Text is generated by iterative denoising rather than next-token prediction.
We use a D3PM-style formulation: randomly mask tokens and learn to reconstruct the original sentence.

## Why this matters for watermarking

In D3PM, token probabilities are computed at every denoising step.
Small, controlled biases can be injected to embed a watermark while keeping text natural.

## What is a BERT tokenizer?

BERT provides a vocabulary, a tokenizer (word → ID), and special tokens.
We need the [MASK] token to represent noise in diffusion.

## Usage examples (what they do + example output)

### 1) Quick smoke-test training

**Command**

```
python models/diffusion_lm.py train \
	--epochs 1 \
	--batch_size 2 \
	--max_length 32 \
	--steps 5 \
	--max_train_batches 2 \
	--device cpu
```

**What it does**

- Runs a tiny CPU training loop (2 batches).
- Verifies data loading, masking, and the training step.

**Example output**

```
Stopping early: max_train_batches=2
Saved checkpoints to: checkpoints/bert-mlm-diffusion-baseline
```

### 2) Sample text (D3PM reverse)

**Command**

```
python models/diffusion_lm.py sample \
	--ckpt checkpoints/bert-mlm-diffusion-baseline/epoch-1 \
	--length 48 \
	--steps 25 \
	--temperature 1.0 \
	--top_k 50
```

**What it does**

- Generates a 48‑token sample using the D3PM reverse process.
- Uses temperature sampling and top‑k filtering.

**Note**

- `bert-base-uncased` is a baseline and not trained with diffusion noise.
- It will run, but it is not a correct diffusion sample.

**Example output**

```
and, ' me and her and mrs. all not foury.
```

### 3) Inspect forward masking

**Command**

```
python models/diffusion_lm.py inspect-mask \
	--split train \
	--idx 0 \
	--max_length 24 \
	--steps 5 \
	--min_mask_prob 0.2 \
	--max_mask_prob 0.8 \
	--t 4
```

**What it does**

- Shows how the forward process masks tokens at a fixed timestep $t$.
- Uses a real dataset example for visibility.

**Example output**

```
split=train idx=0 max_length=24
t=4/5  p_t=0.650  masked=6/7

ORIGINAL (decoded):
= valkyria chronicles iii =

MASKED (decoded):
chronicles

TOKENS (original):
['[CLS]', '=', 'val', '##ky', '##ria', 'chronicles', 'iii', '=', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']

TOKENS (masked):
['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'chronicles', '[MASK]', '[MASK]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
```
