What is watermarking?

Watermarking is a technique for embedding a hidden, statistically detectable signal into the text generated by a language model.
The goal is to make it possible to verify whether a piece of text generated by a AI system, while keeping the text fluent and natural to human readers.
It is protecting models from various intellectual property violations like copyright infringement, unauthorized use and modification of the model.
Model owners must be aware of these potential risks.

What are diffusion models (DMs)?

They are generative models that generate realistic images or text through a sequential denoising process.
Unlike autoregressive models such as GPTs, which generate text one token at a time, diffusion models generate and update all tokens jointly over multiple steps, allowing global revision and refinement.
But they are not secure by design. They may be susceptible to potential misuse, theft of intellectual property.
This can lead to loss of revenue and reputation of the owner. It may enable the attacker to abuse the model such as generating fake images or videos.

Why watermark diffusion models?

As AI-generated text becomes widespread, it becomes increasingly difficult to distinguish human-written content from AI-generated content.
Watermarking presents a viable solution by embedding a unique identifier or signature into the text such that there is minimal quality degradation while ensuring robustness and strong guarantees against removal or evasion.

Goal?

Diffusion-based language models generate text thorugh iterative denoising rather than left-to-right sampling, which means that classical watermarking methods do not directly apply.
This project therefore investigated how to design principled watermarking schemes that exploit the structure of diffusion models while respecting fundamental trade-offs between - detectability, robutness, and text quality.